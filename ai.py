# ai.py
import os
from dotenv import load_dotenv
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import datetime

# å¯¼å…¥æœç´¢åŠŸèƒ½
from search import google_custom_search, format_search_results

load_dotenv()

# å¯ç”¨çš„Geminiæ¨¡å‹åˆ—è¡¨
AVAILABLE_MODELS = {
    # ================ 1.5 ç³»åˆ—æ¨¡å‹ ================
    # Flash æ¨¡å‹ (æ›´å¿«é€Ÿ)
    "gemini-1.5-flash-latest": {
        "description": "1.5 Flash æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬ï¼Œæ›´å¿«é€Ÿï¼Œé€‚åˆä¸€èˆ¬é—®ç­”",
        "max_tokens": 8192,
        "supports_tools": True
    },
    "gemini-1.5-flash": {
        "description": "1.5 Flash åŸºç¡€æ¨¡å‹",
        "max_tokens": 8192,
        "supports_tools": True
    },
    "gemini-1.5-flash-001": {
        "description": "1.5 Flash çš„ 001 ç‰ˆæœ¬",
        "max_tokens": 8192,
        "supports_tools": True
    },
    "gemini-1.5-flash-002": {
        "description": "1.5 Flash çš„ 002 ç‰ˆæœ¬",
        "max_tokens": 8192,
        "supports_tools": True
    },
    "gemini-1.5-flash-8b": {
        "description": "1.5 Flash 8B å°å‹æ¨¡å‹",
        "max_tokens": 8192,
        "supports_tools": True
    },
    "gemini-1.5-flash-8b-latest": {
        "description": "1.5 Flash 8B æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬",
        "max_tokens": 8192,
        "supports_tools": True
    },
    
    # Pro æ¨¡å‹ (æ›´å¼ºå¤§)
    "gemini-1.5-pro-latest": {
        "description": "1.5 Pro æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬ï¼Œæ›´å¼ºå¤§çš„æ¨ç†èƒ½åŠ›",
        "max_tokens": 8192,
        "supports_tools": True
    },
    "gemini-1.5-pro": {
        "description": "1.5 Pro åŸºç¡€æ¨¡å‹",
        "max_tokens": 8192,
        "supports_tools": True
    },
    "gemini-1.5-pro-001": {
        "description": "1.5 Pro çš„ 001 ç‰ˆæœ¬",
        "max_tokens": 8192,
        "supports_tools": True
    },
    "gemini-1.5-pro-002": {
        "description": "1.5 Pro çš„ 002 ç‰ˆæœ¬",
        "max_tokens": 8192,
        "supports_tools": True
    },
    
    # ================ 2.5 ç³»åˆ—æ¨¡å‹ (é¢„è§ˆç‰ˆ) ================
    "gemini-2.5-flash-preview-05-20": {
        "description": "2.5 Flash æœ€æ–°é¢„è§ˆç‰ˆæœ¬ (2025-05-20)",
        "max_tokens": 8192,
        "supports_tools": True
    },
    "gemini-2.5-pro-preview-05-06": {
        "description": "2.5 Pro æœ€æ–°é¢„è§ˆç‰ˆæœ¬ (2025-05-06)",
        "max_tokens": 8192,
        "supports_tools": True
    },
    "gemini-2.5-flash-preview-tts": {
        "description": "2.5 Flash æ”¯æŒæ–‡æœ¬è½¬è¯­éŸ³çš„é¢„è§ˆç‰ˆæœ¬",
        "max_tokens": 8192,
        "supports_tools": True
    },
    "gemini-2.5-pro-preview-tts": {
        "description": "2.5 Pro æ”¯æŒæ–‡æœ¬è½¬è¯­éŸ³çš„é¢„è§ˆç‰ˆæœ¬",
        "max_tokens": 8192,
        "supports_tools": True
    },
    
    # ================ 1.0 ç³»åˆ—æ¨¡å‹ ================
    "gemini-pro": {
        "description": "Gemini 1.0 Pro æ¨¡å‹ï¼Œæ”¯æŒæ–‡æœ¬å¤„ç†",
        "max_tokens": 2048,
        "supports_tools": True
    },
    "gemini-pro-vision": {
        "description": "Gemini 1.0 Pro Vision æ¨¡å‹ï¼Œæ”¯æŒå›¾åƒå¤„ç†",
        "max_tokens": 2048,
        "supports_tools": False
    },
    
    # ================ åµŒå…¥æ¨¡å‹ ================
    "embedding-gecko-001": {
        "description": "PaLM 2 åµŒå…¥æ¨¡å‹",
        "max_tokens": 768,  # åµŒå…¥ç»´åº¦
        "supports_tools": False
    },
    "gemini-embedding-exp": {
        "description": "Gemini åµŒå…¥æ¨¡å‹ï¼ˆå®éªŒç‰ˆï¼‰",
        "max_tokens": 768,  # åµŒå…¥ç»´åº¦å¯èƒ½æ›´é«˜
        "supports_tools": False
    },
}

def get_available_free_models():
    """
    è·å–å½“å‰æ‰€æœ‰å¯ç”¨çš„å…è´¹æ¨¡å‹
    
    Returns:
        dict: ä»¥æ¨¡å‹åç§°ä¸ºé”®ï¼Œæ¨¡å‹ç±»å‹ä¸ºå€¼çš„å­—å…¸
    """
    try:
        # è·å–APIå¯†é’¥
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            print("æœªé…ç½® GEMINI_API_KEY")
            return {}
            
        genai.configure(api_key=api_key)
        
        # è·å–æ‰€æœ‰æ¨¡å‹
        all_models = genai.list_models()
        
        # å¯èƒ½çš„å…è´¹æ¨¡å‹å‰ç¼€æˆ–æ ‡è¯†ç¬¦
        free_tier_identifiers = [
            "bison-001",           # PaLM2 æ–‡æœ¬/èŠå¤©
            "gecko-001",           # PaLM2 åµŒå…¥
            "gemini-2.5-flash",    # Gemini 2.5 Flash é¢„è§ˆ
            "gemini-2.5-pro",      # Gemini 2.5 Pro é¢„è§ˆ
            "gemini-embedding",    # Gemini åµŒå…¥
            "gemini-1.5-flash",    # Gemini 1.5 Flash
            "gemini-1.5-pro",      # Gemini 1.5 Pro
            "gemini-pro",          # Gemini Pro
        ]
        
        # æŒ‰åŠŸèƒ½åˆ†ç±»å­˜å‚¨æ¨¡å‹
        model_by_type = {
            "text": [],
            "chat": [],
            "embedding": [],
            "multimodal": [],
            "other": []
        }
        
        # ç­›é€‰å¯èƒ½çš„å…è´¹æ¨¡å‹
        free_models = []
        for model in all_models:
            model_name = model.name
            # æå–æ¨¡å‹å®é™…åç§°ï¼ˆå»é™¤è·¯å¾„å‰ç¼€ï¼‰
            short_name = model_name.split('/')[-1] if '/' in model_name else model_name
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¯èƒ½çš„å…è´¹æ¨¡å‹
            if any(identifier in short_name for identifier in free_tier_identifiers):
                free_models.append(short_name)
                
                # æ ¹æ®åç§°æˆ–åŠŸèƒ½åˆ†ç±»
                if "embedding" in short_name:
                    model_by_type["embedding"].append(short_name)
                elif "vision" in short_name or "multimodal" in short_name:
                    model_by_type["multimodal"].append(short_name)
                elif "chat" in short_name:
                    model_by_type["chat"].append(short_name)
                elif any(text_id in short_name for text_id in ["gemini", "pro", "flash", "bison"]):
                    model_by_type["text"].append(short_name)
                else:
                    model_by_type["other"].append(short_name)
        
        return model_by_type
    
    except Exception as e:
        print(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
        return {}


def list_free_models():
    """åˆ—å‡ºå¹¶æ‰“å°æ‰€æœ‰å¯ç”¨çš„å…è´¹æ¨¡å‹"""
    model_by_type = get_available_free_models()
    
    if not any(model_by_type.values()):
        print("æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨æ¨¡å‹æˆ–APIè°ƒç”¨å¤±è´¥")
        return
    
    print("\nå½“å‰å¯ç”¨çš„å…è´¹æ¨¡å‹ï¼š")
    print("-" * 80)
    
    for model_type, models in model_by_type.items():
        if models:
            print(f"\n{model_type.title()} æ¨¡å‹:")
            for model in sorted(models):
                print(f"- {model}")
    
    print("\n" + "-" * 80)
    print("æ³¨æ„ï¼šæ¨¡å‹çš„åˆ†ç±»æ˜¯æ ¹æ®åç§°ç‰¹å¾æ¨æ–­çš„ï¼Œå¯èƒ½ä¸å®Œå…¨å‡†ç¡®")
    print("-" * 80)


def ask_ai(prompt, model_name="gemini-1.5-flash-latest", max_output_tokens=None, enable_search=False, search_results_count=3):
    """
    ä½¿ç”¨ Gemini AI æ¨¡å‹è·å–å¯¹æŒ‡å®šé—®é¢˜çš„å›ç­”
    
    Args:
        prompt (str): å‘AIæé—®çš„å†…å®¹
        model_name (str, optional): ä½¿ç”¨çš„æ¨¡å‹åç§°ã€‚é»˜è®¤ä¸º "gemini-1.5-flash-latest"ã€‚
            å¯ç”¨æ¨¡å‹ï¼šgemini-1.5-pro-latest, gemini-1.5-flash-latest, gemini-pro, gemini-pro-vision
        max_output_tokens (int, optional): æœ€å¤§è¾“å‡ºä»¤ç‰Œæ•°ã€‚å¦‚æœä¸æŒ‡å®šï¼Œå°†æ ¹æ®æ¨¡å‹è‡ªåŠ¨è®¾ç½®ã€‚
        enable_search (bool, optional): æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢åŠŸèƒ½ã€‚é»˜è®¤ä¸ºFalseã€‚
        search_results_count (int, optional): å¦‚æœå¯ç”¨æœç´¢ï¼ŒæŒ‡å®šè¿”å›çš„æœç´¢ç»“æœæ•°é‡ã€‚
    """
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print("æœªé…ç½® GEMINI_API_KEY")
        return

    try:
        genai.configure(api_key=api_key)

        # é…ç½®å®‰å…¨è®¾ç½®
        safety_settings = [
            {
                "category": HarmCategory.HARM_CATEGORY_HARASSMENT,
                "threshold": HarmBlockThreshold.BLOCK_NONE,
            },
            {
                "category": HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                "threshold": HarmBlockThreshold.BLOCK_NONE,
            },
            {
                "category": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                "threshold": HarmBlockThreshold.BLOCK_NONE,
            },
            {
                "category": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                "threshold": HarmBlockThreshold.BLOCK_NONE,
            },
        ]

        # é…ç½®ç”Ÿæˆå‚æ•°
        generation_config = {
            "temperature": 0.7,
            "top_p": 0.95,
            "top_k": 60,
            "max_output_tokens": 8192,
        }

        # è·å–æ¨¡å‹ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        model_info = AVAILABLE_MODELS.get(model_name, {
            "description": "æœªçŸ¥æ¨¡å‹",
            "max_tokens": 8192,
            "supports_tools": True
        })
        
        # æ ¹æ®æ¨¡å‹é…ç½®ä¿®æ”¹æœ€å¤§è¾“å‡ºä»¤ç‰Œæ•°
        if not max_output_tokens:
            generation_config["max_output_tokens"] = model_info["max_tokens"]
            
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = genai.GenerativeModel(
            model_name=model_name,
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        
        print(f"ä½¿ç”¨æ¨¡å‹: {model_name} - {model_info['description']}")
        print(f"æœ€å¤§è¾“å‡ºä»¤ç‰Œæ•°: {generation_config['max_output_tokens']}")

        print(f"\nğŸ¤– å‘AIæé—®: {prompt}")
        print("-" * 80)
        
        # å¦‚æœå¯ç”¨æœç´¢åŠŸèƒ½ï¼Œå…ˆæ‰§è¡Œæœç´¢å¹¶å¢å¼ºæç¤º
        search_info = ""
        if enable_search:
            # é’ˆå¯¹æ–°é—»æŸ¥è¯¢ä¼˜åŒ–æœç´¢å…³é”®è¯
            search_query = prompt
            if "æ–°é—»" in prompt or "TOP" in prompt.upper() or "çƒ­ç‚¹" in prompt:
                current_date = datetime.datetime.now().strftime("%Yå¹´%mæœˆ")
                
                # è®¡ç®—è¿‡å»ä¸‰å¤©çš„æ—¥æœŸèŒƒå›´
                today = datetime.datetime.now()
                three_days_ago = (today - datetime.timedelta(days=3)).strftime("%Y-%m-%d")
                today_str = today.strftime("%Y-%m-%d")
                date_range = f"after:{three_days_ago} before:{today_str}"
                
                # ç‰¹åˆ«å…³æ³¨è´¢ç»ã€å†›äº‹å’Œå›½é™…é‡å¤§æ–°é—»ï¼Œé™åˆ¶ä¸‰å¤©å†…
                search_query = f"(è´¢ç»|é‡‘è|ç»æµ|å†›äº‹|å›½é™…å¤§äº‹|å…¨çƒæ–°é—») çƒ­ç‚¹ é‡è¦ {date_range} site:ft.com OR site:wsj.com OR site:bloomberg.com OR site:reuters.com OR site:news.cn OR site:bbc.com OR site:cnn.com OR site:military.com"
                print(f"ä¼˜åŒ–æ–°é—»æœç´¢å…³é”®è¯ (æœ€è¿‘3å¤©): '{search_query}'")                
                # æŒ‰ç”¨æˆ·åå¥½å¢åŠ è´¢ç»æ–°é—»æœç´¢(æœ€è¿‘3å¤©)
                finance_search_query = f"(å…¨çƒè´¢ç»|é‡‘èå¸‚åœº|è‚¡å¸‚|æ±‡ç‡|ç»æµæ–°é—») é‡è¦ {date_range} site:ft.com OR site:wsj.com OR site:bloomberg.com OR site:cnbc.com"
                print(f"æœç´¢è´¢ç»æ–°é—» (æœ€è¿‘3å¤©)...")
                finance_results = google_custom_search(finance_search_query, max(3, search_results_count//3))
                
                # æŒ‰ç”¨æˆ·åå¥½å¢åŠ å†›äº‹æ–°é—»æœç´¢(æœ€è¿‘3å¤©)
                military_search_query = f"(å†›äº‹æ–°é—»|å†›äº‹å†²çª|å®‰å…¨å±€åŠ¿|æˆ˜äº‰|å›½é˜²) é‡è¦ {date_range} site:military.com OR site:defenseone.com OR site:news.cn OR site:bbc.com"
                print(f"æœç´¢å†›äº‹æ–°é—» (æœ€è¿‘3å¤©)...")
                military_results = google_custom_search(military_search_query, max(3, search_results_count//3))
                
            print(f"æ­£åœ¨è¿›è¡Œç½‘ç»œæœç´¢...")
            search_results = google_custom_search(search_query, search_results_count)
            
            if search_results and search_results.get('items'):
                search_info = format_search_results(search_results, search_results_count)
                print("æœç´¢å®Œæˆï¼Œæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
                
                # é’ˆå¯¹æ–°é—»ç±»æŸ¥è¯¢æ„å»ºä¸“é—¨çš„æç¤º
                if "æ–°é—»" in prompt or "TOP" in prompt.upper() or "çƒ­ç‚¹" in prompt:
                    # æ•´åˆé¢å¤–çš„è´¢ç»å’Œå†›äº‹æ–°é—»æœç´¢ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
                    finance_info = ""
                    if 'finance_results' in locals() and finance_results and finance_results.get('items'):
                        finance_info = "\n### è´¢ç»é‡è¦æ–°é—»\n" + format_search_results(finance_results, max(3, search_results_count//3))
                        
                    military_info = ""
                    if 'military_results' in locals() and military_results and military_results.get('items'):
                        military_info = "\n### å†›äº‹é‡è¦æ–°é—»\n" + format_search_results(military_results, max(3, search_results_count//3))
                    
                    # åˆå¹¶æ‰€æœ‰æœç´¢ç»“æœ
                    all_search_info = search_info + finance_info + military_info
                    
                    enhanced_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆã€‚æˆ‘å°†ä¸ºä½ æä¾›ä¸€äº›æœ€è¿‘ä¸‰å¤©å†…çš„æ–°é—»æœç´¢ç»“æœï¼Œä½ çš„ä»»åŠ¡æ˜¯æ•´åˆè¿™äº›ä¿¡æ¯å¹¶æå–å½“å‰æœ€é‡è¦çš„å…¨çƒæ–°é—»äº‹ä»¶ã€‚

è¿™æ˜¯æœ€è¿‘ä¸‰å¤©çš„æ–°é—»æœç´¢ç»“æœï¼š
{all_search_info}

åŸºäºä»¥ä¸Šæœç´¢ç»“æœå’Œä½ çš„çŸ¥è¯†ï¼Œè¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{prompt}

è¦æ±‚ï¼š
1. åˆ†æå¹¶æå–æœ€è¿‘ä¸‰å¤©å†…æœ€é‡è¦çš„å…¨çƒæ–°é—»äº‹ä»¶ï¼Œç‰¹åˆ«æ³¨é‡è´¢ç»ã€å†›äº‹å’Œé‡å¤§å›½é™…äº‹ä»¶
2. æŒ‰é‡è¦æ€§æ’åºåˆ—å‡ºæ–°é—»æ¡ç›®ï¼Œæ ¹æ®æœ€æ–°æœç´¢ç»“æœç¡®ä¿å†…å®¹çš„æ—¶æ•ˆæ€§
3. æ¯æ¡æ–°é—»ç®€è¦æè¿°å…·ä½“äº‹ä»¶å’Œå½±å“ï¼Œå¹¶æ³¨æ˜å±äºå“ªç±»æ–°é—»ï¼ˆè´¢ç»/å†›äº‹/å›½é™…ï¼‰
4. å°½é‡æä¾›æ–°é—»å‘ç”Ÿçš„å¤§è‡´æ—¶é—´æˆ–æ—¥æœŸ
5. å¦‚æœæœç´¢ç»“æœä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†æ¥è¡¥å……å¯èƒ½çš„é‡è¦æ–°é—»

è¯·ç¡®ä¿ä½ çš„å›ç­”æ¸…æ™°ã€å‡†ç¡®ã€å®¢è§‚ã€æœ‰æ¡ç†ã€‚"""
                else:
                    # ä¸€èˆ¬æŸ¥è¯¢çš„å¢å¼ºæç¤º
                    enhanced_prompt = f"""æˆ‘å°†ä¸ºä½ æä¾›ä¸€äº›æœ€æ–°çš„æœç´¢ç»“æœï¼Œè¯·åŸºäºè¿™äº›ä¿¡æ¯å’Œä½ çš„çŸ¥è¯†å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚

{search_info}

ç°åœ¨è¯·å›ç­”è¿™ä¸ªé—®é¢˜ï¼š{prompt}

è¯·ç»¼åˆä½¿ç”¨æœç´¢ç»“æœå’Œä½ çš„çŸ¥è¯†è¿›è¡Œå›ç­”ï¼Œæ‰€æœ‰å›ç­”éœ€è¦å‡†ç¡®ä¸”åŠæ—¶ã€‚å¦‚æœæœç´¢ç»“æœä¸­æœ‰æ›´åŠæ—¶çš„ä¿¡æ¯ï¼Œè¯·ä»¥æœç´¢ç»“æœä¸ºå‡†ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"""
                prompt = enhanced_prompt
            else:
                print("æ— æ³•è·å–æœç´¢ç»“æœæˆ–æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
        
        # ç”Ÿæˆå†…å®¹
        response = model.generate_content(
            prompt,
            stream=True
        )

        full_response = ""
        for chunk in response:
            if chunk.text:
                print(chunk.text, end="", flush=True)
                full_response += chunk.text
        print("\n" + "-" * 80)
        
        return full_response

    except Exception as e:
        print(f"AIå›ç­”å¤±è´¥: {str(e)}")
        return None

if __name__ == "__main__":
    # åˆ—å‡ºå½“å‰å¯ç”¨çš„å…è´¹æ¨¡å‹
    # list_free_models()
    
    # ä½¿ç”¨ç‰¹å®šæ¨¡å‹è¿›è¡Œæé—® (æ ¹æ®å®é™…å¯ç”¨æ¨¡å‹è°ƒæ•´)
    # ä¸å¯ç”¨è”ç½‘æœç´¢çš„åŸºæœ¬ç”¨æ³•
    # ask_ai("ä½ èƒ½ä½¿ç”¨å“ªäº›å·¥å…·è¿›è¡Œè”ç½‘æœç´¢ï¼Ÿ", model_name="gemini-2.5-flash-preview-05-20")
    
    # å¯ç”¨è”ç½‘æœç´¢çš„å¢å¼ºç”¨æ³•
    ask_ai("æˆªæ­¢è‡³å½“å‰ï¼Œå…¨çƒå¤§æ–°é—»TOP10", 
          model_name="gemini-1.5-flash-latest", 
          enable_search=True, 
          search_results_count=10)